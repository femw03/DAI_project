{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lane segmentation\n",
    "This notebooks contains the code we used to train image segmentation to detect the lane markers from an RGB image\n",
    "\n",
    "### 1. Prepare data\n",
    "From carla we obtained RGB images and their respective segmentation image. We need to convert those images into a binary mask and create a dataset for them. To this end we require the following structure before hand:\n",
    "- data/\n",
    "    - raw/\n",
    "        - rgb_images/\n",
    "        - segmentation_images/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "import os\n",
    "import sys\n",
    "\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "import wandb\n",
    "%autoreload 2\n",
    "from DAI.cv.traffic_lane_segmentation import TrafficLaneSegmentationDataModule, TrafficLaneSegmentationModel\n",
    "logger.remove()\n",
    "logger.add(lambda msg: tqdm.write(msg, end=''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Soja\\1_Masters\\AI Lab\\AI project\\Lane detect integration\\DAI_project\\data\n"
     ]
    }
   ],
   "source": [
    "data_root = os.path.abspath(\"../data\") ## This should point to the data directory as described above.\n",
    "print(data_root)\n",
    "os.makedirs(os.path.join(data_root, 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(data_root, 'labels'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 881/881 [01:51<00:00,  7.88it/s]\n"
     ]
    }
   ],
   "source": [
    "traffic_lane_color = np.array(([157, 234, 50]))\n",
    "\n",
    "image_set_length = len(os.listdir(os.path.join(data_root, 'raw', 'rgb_images')))\n",
    "for image_name in tqdm(os.listdir(os.path.join(data_root, 'raw', 'rgb_images'))):\n",
    "    image_number = re.findall('image(\\d+).png', image_name)[0]\n",
    "    rgb_image_path = os.path.join(data_root, 'raw', 'rgb_images', image_name)\n",
    "    seg_image_path = os.path.join(data_root, 'raw', 'segmentation_images',f'seg{image_number}.png')\n",
    "    try:\n",
    "        rgb_image = cv2.imread(rgb_image_path)\n",
    "        seg_image = cv2.imread(seg_image_path)\n",
    "    except Exception: \n",
    "        continue\n",
    "    \n",
    "    if rgb_image is None or seg_image is None:\n",
    "        logger.warning(f\"skipping {rgb_image_path}\")\n",
    "        continue\n",
    "    mask: np.ndarray = np.any(seg_image == traffic_lane_color, axis=-1)\n",
    "    binary_segmentation_image = (mask*255).astype(np.uint8)\n",
    "    cv2.imwrite(os.path.join(data_root, 'images', image_name), rgb_image)\n",
    "    cv2.imwrite(os.path.join(data_root, 'labels', f\"segm{image_number}.png\"), binary_segmentation_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Prepare Model\n",
    "We use the torchvision DeepLabV3 segmentation model which we wrapped with a lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss</td><td>█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▂▂▁▁▂▂▂▂▂▂▁▂▂▁▂▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>val_loss</td><td>▁█▂▁▁▁▁▁▁▅▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>13</td></tr><tr><td>train_loss</td><td>0.00052</td></tr><tr><td>trainer/global_step</td><td>4815</td></tr><tr><td>val_loss</td><td>0.00411</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deeplabv3_mobilenet_v3_large</strong> at: <a href='https://wandb.ai/sojalavin-universiteit-antwerpen/Traffic-lane-segmentation/runs/lggw4ca4' target=\"_blank\">https://wandb.ai/sojalavin-universiteit-antwerpen/Traffic-lane-segmentation/runs/lggw4ca4</a><br/> View project at: <a href='https://wandb.ai/sojalavin-universiteit-antwerpen/Traffic-lane-segmentation' target=\"_blank\">https://wandb.ai/sojalavin-universiteit-antwerpen/Traffic-lane-segmentation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241204_180334-lggw4ca4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "data_loader = TrafficLaneSegmentationDataModule(data_root, batch_size=2)\n",
    "model = TrafficLaneSegmentationModel()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Train model\n",
    "We setup training with Weights and biases early stopping and model checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\Soja\\1_Masters\\AI Lab\\AI project\\Lane detect integration\\DAI_project\\.venv\\lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20241204_180334-lggw4ca4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sojalavin-universiteit-antwerpen/Traffic-lane-segmentation/runs/lggw4ca4' target=\"_blank\">deeplabv3_mobilenet_v3_large</a></strong> to <a href='https://wandb.ai/sojalavin-universiteit-antwerpen/Traffic-lane-segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sojalavin-universiteit-antwerpen/Traffic-lane-segmentation' target=\"_blank\">https://wandb.ai/sojalavin-universiteit-antwerpen/Traffic-lane-segmentation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sojalavin-universiteit-antwerpen/Traffic-lane-segmentation/runs/lggw4ca4' target=\"_blank\">https://wandb.ai/sojalavin-universiteit-antwerpen/Traffic-lane-segmentation/runs/lggw4ca4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Soja\\1_Masters\\AI Lab\\AI project\\Lane detect integration\\DAI_project\\.venv\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory D:\\Soja\\1_Masters\\AI Lab\\AI project\\Lane detect integration\\DAI_project\\notebooks\\models exists and is not empty.\n",
      "\n",
      "  | Name  | Type              | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | model | DeepLabV3         | 11.0 M | train\n",
      "1 | loss  | BCEWithLogitsLoss | 0      | train\n",
      "----------------------------------------------------\n",
      "11.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.0 M    Total params\n",
      "44.081    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 344/344 [01:31<00:00,  3.75it/s, v_num=4ca4]    \n"
     ]
    }
   ],
   "source": [
    "save_callback = ModelCheckpoint('./models', 'best.pt', monitor='val_loss')\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=5)\n",
    "logging = WandbLogger(project='Traffic-lane-segmentation', name='lraspp_mobilenet_v3_large')\n",
    "trainer = L.Trainer(accelerator='auto', callbacks=[save_callback, early_stopper], logger=logging)\n",
    "trainer.fit(model, datamodule=data_loader, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
